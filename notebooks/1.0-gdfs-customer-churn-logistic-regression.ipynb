{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Predicting Customer Churn With Logistic Regression\n","\n","* Analysis performed using purely PySpark;\n","* Churn dataset with 10,000 instances. Instances are randomly split, 70% for training and 30% for testing;\n","* Two scenarios evaluated: one without compensating for class imbalance and one compensating for it;\n","* Class imbalance dealt with using class weights in logistic regression;\n","* Two models created, one for each scenario. Models evaluated using accuracy, weighted precision, weighted recall, AUCROC, AUCPR and F1-score;\n","* Dataset available online at: https://www.kaggle.com/code/mathchi/churn-problem-for-bank-customer"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset Summary\n","\n","The dataset is composed of information from bank customers with the response variable \"Exited\" indicating whether the customer left the bank or not. The table below provides some details on each one of the attributes.\n","\n","| Attribute | Summary |\n","|:---------:|:-------:|\n","| CreditScore | Credit score calculated by the bank. |\n","| Geography | Country where the customer resides: Germany, France or Spain. |\n","| Gender | Gender between Female and Male. |\n","| Age | Customer's age. |\n","| Tenure | Amount of years the customer has been with the bank. |\n","| Balance | Current balance. |\n","| NumOfProducts | Number of bank products used. |\n","| HasCrCard | Credit card status (0: does not have; 1: has credit card). |\n","| IsActiveMember | Active membership status (0: not active; 1: active). |\n","| EstimatedSalary | Customer's estimated salary. |\n","| Exited | Customer status (0: has not left the bank; 1: has left the bank). |\n"]},{"cell_type":"code","execution_count":2,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"6a1d3d1c-c56a-4a16-bd91-73e11972009f","showTitle":false,"title":""}},"outputs":[],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import round, udf\n","from pyspark.sql.types import DoubleType\n","from pyspark.ml.classification import LogisticRegression\n","from pyspark.ml.feature import RFormula, VectorAssembler, StringIndexer\n","from pyspark.ml.stat import Correlation\n","from pyspark.ml.evaluation import (BinaryClassificationEvaluator,\n","                                   MulticlassClassificationEvaluator)"]},{"cell_type":"markdown","metadata":{},"source":["## Data Exploration and Preprocessing\n","\n","We aim to create a model capable of predicting if a customer will end his account on a bank or not. Since this is inherently a binary classification task, a logistic regressor is a great candidate for it.\n","\n","After loading our churn dataset into a PySpark dataframe, we will print it and count the number of instances to get a notion of our data. With 10,000 instances, a reasonable number for the task, we will not need data augmentation. There are 10 predictor variables and one response variable: \"Exited\".\n","\n","Looking at the response variable for the 20 records printed, it is already possible to see that there is class imbalance in this dataset. This will lead us to two scenarios: in the first one, we will not compensate for class imbalance; in the second one, we will fix the class imbalance."]},{"cell_type":"code","execution_count":3,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"2e0ca765-b6f7-4578-bebc-4c4a3e0cd6c2","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of instances in Churn dataset:  10000\n","+-----------+---------+------+---+------+--------+-------------+---------+--------------+---------------+------+\n","|CreditScore|Geography|Gender|Age|Tenure| Balance|NumOfProducts|HasCrCard|IsActiveMember|EstimatedSalary|Exited|\n","+-----------+---------+------+---+------+--------+-------------+---------+--------------+---------------+------+\n","|        619|   France|Female| 42|     2|       0|            1|        1|             1|       10134888|     1|\n","|        608|    Spain|Female| 41|     1| 8380786|            1|        0|             1|       11254258|     0|\n","|        502|   France|Female| 42|     8| 1596608|            3|        1|             0|       11393157|     1|\n","|        699|   France|Female| 39|     1|       0|            2|        0|             0|        9382663|     0|\n","|        850|    Spain|Female| 43|     2|12551082|            1|        1|             1|         790841|     0|\n","|        645|    Spain|  Male| 44|     8|11375578|            2|        1|             0|       14975671|     1|\n","|        822|   France|  Male| 50|     7|       0|            2|        1|             1|         100628|     0|\n","|        376|  Germany|Female| 29|     4|11504674|            4|        1|             0|       11934688|     1|\n","|        501|   France|  Male| 44|     4|14205107|            2|        0|             1|         749405|     0|\n","|        684|   France|  Male| 27|     2|13460388|            1|        1|             1|        7172573|     0|\n","|        528|   France|  Male| 31|     6|10201672|            2|        0|             0|        8018112|     0|\n","|        497|    Spain|  Male| 24|     3|       0|            2|        1|             0|        7639001|     0|\n","|        476|   France|Female| 34|    10|       0|            2|        1|             0|        2626098|     0|\n","|        549|   France|Female| 25|     5|       0|            2|        0|             0|       19085779|     0|\n","|        635|    Spain|Female| 35|     7|       0|            2|        1|             1|        6595165|     0|\n","|        616|  Germany|  Male| 45|     3|14312941|            2|        0|             1|        6432726|     0|\n","|        653|  Germany|  Male| 58|     1|13260288|            1|        1|             0|         509767|     1|\n","|        549|    Spain|Female| 24|     9|       0|            2|        1|             1|        1440641|     0|\n","|        587|    Spain|  Male| 45|     6|       0|            1|        0|             0|       15868481|     0|\n","|        726|   France|Female| 24|     6|       0|            2|        1|             1|        5472403|     0|\n","+-----------+---------+------+---+------+--------+-------------+---------+--------------+---------------+------+\n","only showing top 20 rows\n","\n"]}],"source":["spark = SparkSession.builder.appName(\"customer_churn_logistic_regression\").getOrCreate()\n","\n","churn = spark.read.load('../data/raw/churn.csv', format='csv', header=True,\n","                        inferSchema=True, sep=';')\n","print(\"Number of instances in Churn dataset: \", churn.count())\n","churn.show()"]},{"cell_type":"markdown","metadata":{},"source":["Our dataset has two string columns that need to be transformed to numeric before we can use them in regression. We use PySpark's `StringIndexer` for that.\n","\n","With data being fully numeric, we can create a correlation matrix to check for variables correlated with the response variable or for correlation between predictor variables."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+------+------+-------+-------------+---------+--------------+---------------+------+--------------+-----------+\n","|CreditScore|   Age|Tenure|Balance|NumOfProducts|HasCrCard|IsActiveMember|EstimatedSalary|Exited|GeographyIndex|GenderIndex|\n","+-----------+------+------+-------+-------------+---------+--------------+---------------+------+--------------+-----------+\n","|        1.0|-0.004| 0.001|  0.007|        0.012|   -0.005|         0.026|         -0.001|-0.027|         0.008|      0.003|\n","|     -0.004|   1.0| -0.01|  0.022|       -0.031|   -0.012|         0.085|         -0.015| 0.285|         0.023|      0.028|\n","|      0.001| -0.01|   1.0| -0.017|        0.013|    0.023|        -0.028|          0.006|-0.014|         0.004|     -0.015|\n","|      0.007| 0.022|-0.017|    1.0|       -0.276|   -0.011|        -0.011|          0.006| 0.106|         0.063|     -0.007|\n","|      0.012|-0.031| 0.013| -0.276|          1.0|    0.003|          0.01|          0.014|-0.048|         0.004|      0.022|\n","|     -0.005|-0.012| 0.023| -0.011|        0.003|      1.0|        -0.012|         -0.006|-0.007|        -0.009|     -0.006|\n","|      0.026| 0.085|-0.028| -0.011|         0.01|   -0.012|           1.0|         -0.006|-0.156|         0.007|     -0.023|\n","|     -0.001|-0.015| 0.006|  0.006|        0.014|   -0.006|        -0.006|            1.0| 0.003|        -0.005|      0.011|\n","|     -0.027| 0.285|-0.014|  0.106|       -0.048|   -0.007|        -0.156|          0.003|   1.0|         0.036|      0.107|\n","|      0.008| 0.023| 0.004|  0.063|        0.004|   -0.009|         0.007|         -0.005| 0.036|           1.0|     -0.005|\n","|      0.003| 0.028|-0.015| -0.007|        0.022|   -0.006|        -0.023|          0.011| 0.107|        -0.005|        1.0|\n","+-----------+------+------+-------+-------------+---------+--------------+---------------+------+--------------+-----------+\n","\n"]}],"source":["# Create index using the string values of Geography and Gender columns.\n","indexer = StringIndexer(inputCols=['Geography', 'Gender'],\n","                        outputCols=['GeographyIndex', 'GenderIndex'])\n","churn = indexer.fit(churn).transform(churn)\n","churn = churn.drop('Geography', 'Gender')\n","\n","# Assemble every column into a single vector to be input to PySpark's\n","# Correlation class.\n","assembler = VectorAssembler(inputCols=churn.columns, outputCol='corr_features')\n","churn_assembled = assembler.transform(churn).select('corr_features')\n","\n","# Get correlation values and build a new dataframe to work as a\n","# correlation matrix. \n","corr_matrix = Correlation.corr(churn_assembled, 'corr_features')\n","corr_matrix = corr_matrix.collect()[0][corr_matrix.columns[0]].toArray()\n","corr_matrix = spark.createDataFrame(corr_matrix.tolist(), churn.columns)\n","\n","# Every correlation value is rounded for ease of view.\n","corr_matrix.select([round(c, 3).alias(c) for c in corr_matrix.columns]).show()"]},{"cell_type":"markdown","metadata":{},"source":["The main correlations we can see in the matrix happen between variables: \"Age\" and \"Exited\" (0.285); \"Balance\" and \"NumOfProducts\" (-0.276); and \"IsActiveMember\" and \"Exited\" (-0.156). There is no significant correlation between independent variables. This means that they are indeed independent on each other. For this reason, and for the fact that the correlations with the dependent variable are also not very significant, we need to be careful before discarding any variables.\n","\n","For now, we will not discard any variables. Still, that remains a possibility if results are bad.\n","\n","PySpark's models need to take all independent variables together in a single vector. We use the R-language-like `RFormula` to perform this casting. Then, we randomly split the data between training and testing sets."]},{"cell_type":"code","execution_count":5,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"4753500a-15b5-4982-9d07-8bc194e1e20f","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------------------------------------------------+-----+\n","|features                                                    |label|\n","+------------------------------------------------------------+-----+\n","|[619.0,42.0,2.0,0.0,1.0,1.0,1.0,1.0134888E7,0.0,1.0]        |1.0  |\n","|[608.0,41.0,1.0,8380786.0,1.0,0.0,1.0,1.1254258E7,2.0,1.0]  |0.0  |\n","|[502.0,42.0,8.0,1596608.0,3.0,1.0,0.0,1.1393157E7,0.0,1.0]  |1.0  |\n","|[699.0,39.0,1.0,0.0,2.0,0.0,0.0,9382663.0,0.0,1.0]          |0.0  |\n","|[850.0,43.0,2.0,1.2551082E7,1.0,1.0,1.0,790841.0,2.0,1.0]   |0.0  |\n","|[645.0,44.0,8.0,1.1375578E7,2.0,1.0,0.0,1.4975671E7,2.0,0.0]|1.0  |\n","|[822.0,50.0,7.0,0.0,2.0,1.0,1.0,100628.0,0.0,0.0]           |0.0  |\n","|[376.0,29.0,4.0,1.1504674E7,4.0,1.0,0.0,1.1934688E7,1.0,1.0]|1.0  |\n","|[501.0,44.0,4.0,1.4205107E7,2.0,0.0,1.0,749405.0,0.0,0.0]   |0.0  |\n","|[684.0,27.0,2.0,1.3460388E7,1.0,1.0,1.0,7172573.0,0.0,0.0]  |0.0  |\n","|[528.0,31.0,6.0,1.0201672E7,2.0,0.0,0.0,8018112.0,0.0,0.0]  |0.0  |\n","|[497.0,24.0,3.0,0.0,2.0,1.0,0.0,7639001.0,2.0,0.0]          |0.0  |\n","|[476.0,34.0,10.0,0.0,2.0,1.0,0.0,2626098.0,0.0,1.0]         |0.0  |\n","|[549.0,25.0,5.0,0.0,2.0,0.0,0.0,1.9085779E7,0.0,1.0]        |0.0  |\n","|[635.0,35.0,7.0,0.0,2.0,1.0,1.0,6595165.0,2.0,1.0]          |0.0  |\n","|[616.0,45.0,3.0,1.4312941E7,2.0,0.0,1.0,6432726.0,1.0,0.0]  |0.0  |\n","|[653.0,58.0,1.0,1.3260288E7,1.0,1.0,0.0,509767.0,1.0,0.0]   |1.0  |\n","|[549.0,24.0,9.0,0.0,2.0,1.0,1.0,1440641.0,2.0,1.0]          |0.0  |\n","|[587.0,45.0,6.0,0.0,1.0,0.0,0.0,1.5868481E7,2.0,0.0]        |0.0  |\n","|[726.0,24.0,6.0,0.0,2.0,1.0,1.0,5472403.0,0.0,1.0]          |0.0  |\n","+------------------------------------------------------------+-----+\n","only showing top 20 rows\n","\n","Number of training instances:  7024\n","Number of testing instances:  2976\n"]}],"source":["# Exited as dependent variable and every other as independent.\n","r_formula = RFormula(formula=\"Exited ~ .\")\n","churn_rf = r_formula.fit(churn).transform(churn)\n","churn_rf.select('features', 'label').show(truncate=False)\n","\n","churn_train, churn_test = churn_rf.randomSplit([0.7, 0.3])\n","print(\"Number of training instances: \", churn_train.count())\n","print(\"Number of testing instances: \", churn_test.count())"]},{"cell_type":"markdown","metadata":{},"source":["## Model Building and Training\n","\n","We instantiate a PySpark logistic regressor using default configurations. This means that the model will have 0.5 as threshold for classifying its probabilistic output; a maximum of 100 iterations during optimization; convergence tolerance of 1e-6, i.e., loss changes smaller than 1e-6 result in end of training; no weights for different classes; and other minor settings that can be checked on the [docs](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.LogisticRegression.html).\n","\n","After training the model, a series of statistics can be obtained from the `summary` class attribute to display the model performance on training data. Our model achieves good results for all metrics viewed. Note that accuracy is not a good metric for measuring the performance on imbalanced datasets. Therefore, weighted precision, recall and area under the ROC curve (AUCROC) are metrics that deserve more attention.\n","\n","In the context of our application, detecting customer churn for banks, it is essential to predict as many churns as possible. This way, the bank can perform countermeasures to mitigate loss of customers. Being sensitive by having a low number of false negatives, therefore reaching a high recall, is the priority. Being precise by having a low number of false positives, and a high precision, comes second. **Weighted recall and AUCROC are our focus for this problem.** They are metrics more related to sensitivity. "]},{"cell_type":"code","execution_count":6,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"6faf330e-0c0d-4417-94c9-7af4db29143d","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":["Model evaluation on training set:\n","Accuracy:  0.8089407744874715\n","Weighted precision:  0.7755397972643587\n","Weighted recall:  0.8089407744874715\n","Area under the ROC curve:  0.7555182326233648\n"]}],"source":["logistic_regressor = LogisticRegression()\n","model = logistic_regressor.fit(churn_train)\n","\n","summary = model.summary\n","print(\"Model evaluation on training set:\")\n","print(\"Accuracy: \", summary.accuracy)\n","print(\"Weighted precision: \", summary.weightedPrecision)\n","print(\"Weighted recall: \", summary.weightedRecall)\n","print(\"Area under the ROC curve: \", summary.areaUnderROC)"]},{"cell_type":"markdown","metadata":{},"source":["For the testing set, the probabilities are on the table below. There is a bias in our model towards predicting no churn (class 0). This is expected since the dataset is imbalanced towards class 0. Our model's performance is impacted by this issue and this reflects on our metrics.\n","\n","Nonetheless, for the performance measures on testing data, we see results similar to those of the training set, indicating good learning generalization.\n","\n","There are two new metrics with reported, F1-score and area under the precision-recall curve (AUCPR). Both are important but AUCROC and weighted recall still need more priority as they focus more on recall (sensitivity).\n","\n","Note that weighted precision and weighted recall take the overall precision and recall measures and weighs them considering the class distribution of the data."]},{"cell_type":"code","execution_count":7,"metadata":{"application/vnd.databricks.v1+cell":{"cellMetadata":{},"inputWidgets":{},"nuid":"204aa6c4-820a-45d4-b1f6-6da73c1f2e59","showTitle":false,"title":""}},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----+----------+----------------------------------------+------------------------------------------+\n","|label|prediction|probability                             |rawPrediction                             |\n","+-----+----------+----------------------------------------+------------------------------------------+\n","|1.0  |0.0       |[0.7870967416486233,0.2129032583513767] |[1.3075132890562,-1.3075132890562]        |\n","|1.0  |0.0       |[0.79610294879415,0.20389705120585]     |[1.3621132946242502,-1.3621132946242502]  |\n","|1.0  |0.0       |[0.860734569830777,0.139265430169223]   |[1.8214044938510021,-1.8214044938510021]  |\n","|1.0  |0.0       |[0.8278527405299569,0.17214725947004306]|[1.570485018739304,-1.570485018739304]    |\n","|1.0  |0.0       |[0.8069897389231682,0.19301026107683183]|[1.4305675994072864,-1.4305675994072864]  |\n","|1.0  |0.0       |[0.7745402044110802,0.22545979558891982]|[1.2341277156836554,-1.2341277156836554]  |\n","|1.0  |0.0       |[0.9275317176435545,0.07246828235644553]|[2.5493780094195837,-2.5493780094195837]  |\n","|1.0  |0.0       |[0.601076681406773,0.39892331859322705] |[0.40995330164818267,-0.40995330164818267]|\n","|1.0  |1.0       |[0.33224774880139757,0.6677522511986025]|[-0.6980363013500579,0.6980363013500579]  |\n","|0.0  |0.0       |[0.8099611464395537,0.19003885356044625]|[1.4497577354792766,-1.4497577354792766]  |\n","|0.0  |0.0       |[0.871637603798466,0.128362396201534]   |[1.915516261663813,-1.915516261663813]    |\n","|0.0  |0.0       |[0.782899912890224,0.21710008710977602] |[1.282646384521541,-1.282646384521541]    |\n","|0.0  |0.0       |[0.7264877394765296,0.27351226052347044]|[0.9768751576205914,-0.9768751576205914]  |\n","|0.0  |0.0       |[0.8749549919797037,0.12504500802029628]|[1.9454987106400408,-1.9454987106400408]  |\n","|1.0  |0.0       |[0.9393965050437756,0.06060349495622441]|[2.7408850892154746,-2.7408850892154746]  |\n","|0.0  |0.0       |[0.9328087649738996,0.06719123502610036]|[2.6306574041019264,-2.6306574041019264]  |\n","|0.0  |0.0       |[0.8515431265178314,0.14845687348216863]|[1.7467556447188213,-1.7467556447188213]  |\n","|0.0  |0.0       |[0.9288360395792405,0.07116396042075945]|[2.568945715531167,-2.568945715531167]    |\n","|0.0  |0.0       |[0.8961164368214077,0.10388356317859226]|[2.1547996692246545,-2.1547996692246545]  |\n","|1.0  |0.0       |[0.9527731501394753,0.04722684986052472]|[3.004414253856451,-3.004414253856451]    |\n","+-----+----------+----------------------------------------+------------------------------------------+\n","only showing top 20 rows\n","\n"]}],"source":["pred = model.transform(churn_test)\n","pred.select('label', 'prediction', 'probability', 'rawPrediction').show(truncate=False)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model evaluation on testing set:\n","Area under the ROC curve: 0.754717290402596\n","Area under the PR curve: 0.44706285563820014\n","Accuracy: 0.8020833333333334\n","Weighted precision: 0.7690886987237457\n","Weighted recall: 0.8020833333333334\n","F1-score: 0.7540965467365904\n"]}],"source":["binary_evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n","print(\"Model evaluation on testing set:\")\n","print(\"Area under the ROC curve:\", binary_evaluator.evaluate(pred))\n","binary_evaluator.setMetricName('areaUnderPR')\n","print(\"Area under the PR curve:\", binary_evaluator.evaluate(pred))\n","\n","multiclass_evaluator = MulticlassClassificationEvaluator(metricName='accuracy')\n","print(\"Accuracy:\", multiclass_evaluator.evaluate(pred))\n","multiclass_evaluator.setMetricName('weightedPrecision')\n","print(\"Weighted precision:\", multiclass_evaluator.evaluate(pred))\n","multiclass_evaluator.setMetricName('weightedRecall')\n","print(\"Weighted recall:\", multiclass_evaluator.evaluate(pred))\n","multiclass_evaluator.setMetricName('f1')\n","print(\"F1-score:\", multiclass_evaluator.evaluate(pred))"]},{"cell_type":"markdown","metadata":{},"source":["## Second Scenario: Compensating for Class Imbalance\n","\n","Since our results are far from perfect, we have to go further with our analysis. The two most clear options to follow are:\n","1. Perform feature selection by dropping independent variables less correlated with the dependent variable;\n","2. Account for class imbalance.\n","\n","The table below shows that, as we observed before, class imbalance in this dataset is significant. There are 4 times more elements of class 0 than elements of class 1. This made our first model have a bias for class 0. As class imbalance is much more significant than variable correlation, we go with the second option."]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+------+-----+\n","|Exited|count|\n","+------+-----+\n","|     1| 2037|\n","|     0| 7963|\n","+------+-----+\n","\n"]}],"source":["churn.groupBy('Exited').count().show()"]},{"cell_type":"markdown","metadata":{},"source":["We have two options for handling class imbalance:\n","1. Resample the dataset so that we end up with the same amount of class 0 and class 1 elements in our training and testing sets;\n","2. Attribute weights for each class before training so that the minority class gets more attention from the model.\n","\n","Fortunately for us, logistic regression can easily work with class weights and PySpark's regression models support this. Option 2 is also preferred because we can avoid having to simulate data for the minority class or having to undersample the majority class. In the end, we still work with the original dataset.\n","\n","The \"ClassWeightCol\", presented in the next table, will provide the weights for each element to our new logistic regressor. Now, class 0 will not overshadow class 1."]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+---+------+--------+-------------+---------+--------------+---------------+------+--------------+-----------+--------------+\n","|CreditScore|Age|Tenure| Balance|NumOfProducts|HasCrCard|IsActiveMember|EstimatedSalary|Exited|GeographyIndex|GenderIndex|ClassWeightCol|\n","+-----------+---+------+--------+-------------+---------+--------------+---------------+------+--------------+-----------+--------------+\n","|        619| 42|     2|       0|            1|        1|             1|       10134888|     1|           0.0|        1.0|        0.7963|\n","|        608| 41|     1| 8380786|            1|        0|             1|       11254258|     0|           2.0|        1.0|        0.2037|\n","|        502| 42|     8| 1596608|            3|        1|             0|       11393157|     1|           0.0|        1.0|        0.7963|\n","|        699| 39|     1|       0|            2|        0|             0|        9382663|     0|           0.0|        1.0|        0.2037|\n","|        850| 43|     2|12551082|            1|        1|             1|         790841|     0|           2.0|        1.0|        0.2037|\n","|        645| 44|     8|11375578|            2|        1|             0|       14975671|     1|           2.0|        0.0|        0.7963|\n","|        822| 50|     7|       0|            2|        1|             1|         100628|     0|           0.0|        0.0|        0.2037|\n","|        376| 29|     4|11504674|            4|        1|             0|       11934688|     1|           1.0|        1.0|        0.7963|\n","|        501| 44|     4|14205107|            2|        0|             1|         749405|     0|           0.0|        0.0|        0.2037|\n","|        684| 27|     2|13460388|            1|        1|             1|        7172573|     0|           0.0|        0.0|        0.2037|\n","|        528| 31|     6|10201672|            2|        0|             0|        8018112|     0|           0.0|        0.0|        0.2037|\n","|        497| 24|     3|       0|            2|        1|             0|        7639001|     0|           2.0|        0.0|        0.2037|\n","|        476| 34|    10|       0|            2|        1|             0|        2626098|     0|           0.0|        1.0|        0.2037|\n","|        549| 25|     5|       0|            2|        0|             0|       19085779|     0|           0.0|        1.0|        0.2037|\n","|        635| 35|     7|       0|            2|        1|             1|        6595165|     0|           2.0|        1.0|        0.2037|\n","|        616| 45|     3|14312941|            2|        0|             1|        6432726|     0|           1.0|        0.0|        0.2037|\n","|        653| 58|     1|13260288|            1|        1|             0|         509767|     1|           1.0|        0.0|        0.7963|\n","|        549| 24|     9|       0|            2|        1|             1|        1440641|     0|           2.0|        1.0|        0.2037|\n","|        587| 45|     6|       0|            1|        0|             0|       15868481|     0|           2.0|        0.0|        0.2037|\n","|        726| 24|     6|       0|            2|        1|             1|        5472403|     0|           0.0|        1.0|        0.2037|\n","+-----------+---+------+--------+-------------+---------+--------------+---------------+------+--------------+-----------+--------------+\n","only showing top 20 rows\n","\n"]}],"source":["# Get the probability for any element to be part of class 1.\n","data_balancing_ratio = (churn.where(churn.Exited == 1).count()\n","                       /  churn.count())\n","\n","# Define an UDF to be used as a dataframe function when providing class\n","# weights. As this a binary classification problem, we can work with the\n","# previous probability and its complement.\n","calculate_weights = udf(lambda x: 1 * data_balancing_ratio if x == 0\n","                        else (1 * (1.0 - data_balancing_ratio)), DoubleType())\n","                        \n","weighted_churn = churn.withColumn('ClassWeightCol', calculate_weights('Exited'))\n","weighted_churn.show()"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------------------------------------------------------------+-----+\n","|features                                                           |label|\n","+-------------------------------------------------------------------+-----+\n","|[619.0,42.0,2.0,0.0,1.0,1.0,1.0,1.0134888E7,0.0,1.0,0.7963]        |1.0  |\n","|[608.0,41.0,1.0,8380786.0,1.0,0.0,1.0,1.1254258E7,2.0,1.0,0.2037]  |0.0  |\n","|[502.0,42.0,8.0,1596608.0,3.0,1.0,0.0,1.1393157E7,0.0,1.0,0.7963]  |1.0  |\n","|[699.0,39.0,1.0,0.0,2.0,0.0,0.0,9382663.0,0.0,1.0,0.2037]          |0.0  |\n","|[850.0,43.0,2.0,1.2551082E7,1.0,1.0,1.0,790841.0,2.0,1.0,0.2037]   |0.0  |\n","|[645.0,44.0,8.0,1.1375578E7,2.0,1.0,0.0,1.4975671E7,2.0,0.0,0.7963]|1.0  |\n","|[822.0,50.0,7.0,0.0,2.0,1.0,1.0,100628.0,0.0,0.0,0.2037]           |0.0  |\n","|[376.0,29.0,4.0,1.1504674E7,4.0,1.0,0.0,1.1934688E7,1.0,1.0,0.7963]|1.0  |\n","|[501.0,44.0,4.0,1.4205107E7,2.0,0.0,1.0,749405.0,0.0,0.0,0.2037]   |0.0  |\n","|[684.0,27.0,2.0,1.3460388E7,1.0,1.0,1.0,7172573.0,0.0,0.0,0.2037]  |0.0  |\n","|[528.0,31.0,6.0,1.0201672E7,2.0,0.0,0.0,8018112.0,0.0,0.0,0.2037]  |0.0  |\n","|[497.0,24.0,3.0,0.0,2.0,1.0,0.0,7639001.0,2.0,0.0,0.2037]          |0.0  |\n","|[476.0,34.0,10.0,0.0,2.0,1.0,0.0,2626098.0,0.0,1.0,0.2037]         |0.0  |\n","|[549.0,25.0,5.0,0.0,2.0,0.0,0.0,1.9085779E7,0.0,1.0,0.2037]        |0.0  |\n","|[635.0,35.0,7.0,0.0,2.0,1.0,1.0,6595165.0,2.0,1.0,0.2037]          |0.0  |\n","|[616.0,45.0,3.0,1.4312941E7,2.0,0.0,1.0,6432726.0,1.0,0.0,0.2037]  |0.0  |\n","|[653.0,58.0,1.0,1.3260288E7,1.0,1.0,0.0,509767.0,1.0,0.0,0.7963]   |1.0  |\n","|[549.0,24.0,9.0,0.0,2.0,1.0,1.0,1440641.0,2.0,1.0,0.2037]          |0.0  |\n","|[587.0,45.0,6.0,0.0,1.0,0.0,0.0,1.5868481E7,2.0,0.0,0.2037]        |0.0  |\n","|[726.0,24.0,6.0,0.0,2.0,1.0,1.0,5472403.0,0.0,1.0,0.2037]          |0.0  |\n","+-------------------------------------------------------------------+-----+\n","only showing top 20 rows\n","\n","Number of training instances:  6946\n","Number of testing instances:  3054\n"]}],"source":["weighted_churn_rf = r_formula.fit(weighted_churn).transform(weighted_churn)\n","weighted_churn_rf.select('features', 'label').show(truncate=False)\n","\n","weighted_churn_train, weighted_churn_test = weighted_churn_rf.randomSplit([0.7, 0.3])\n","print(\"Number of training instances: \", weighted_churn_train.count())\n","print(\"Number of testing instances: \", weighted_churn_test.count())"]},{"cell_type":"markdown","metadata":{},"source":["Looking at the same metrics we saw for the training phase of the first model, we already note a huge boost in performance. All metrics are perfect, even weighted recall and AUCROC, our prioritized metrics. This indicates that the main challenge for this data really is class imbalance. A logistic regressor, even being a simple model, seems to be perfectly able to understand this dataset. Now we check the testing results to confirm this notion and that the model has good generalization capabilities."]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model evaluation on training set:\n","Accuracy:  1.0\n","Weighted precision:  1.0\n","Weighted recall:  1.0\n","Area under the ROC curve:  0.999999483724412\n"]}],"source":["logistic_regressor.setWeightCol('ClassWeightCol')\n","model = logistic_regressor.fit(weighted_churn_train)\n","\n","summary = model.summary\n","print(\"Model evaluation on training set:\")\n","print(\"Accuracy: \", summary.accuracy)\n","print(\"Weighted precision: \", summary.weightedPrecision)\n","print(\"Weighted recall: \", summary.weightedRecall)\n","print(\"Area under the ROC curve: \", summary.areaUnderROC)"]},{"cell_type":"markdown","metadata":{},"source":["In the table below, for the 20 samples shown, our model does not commit a single mistake. All instances are predicted with above 99% probability and with much larger values for raw prediction, also indicating that the model is more certain of its predictions."]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----+----------+------------------------------------------+----------------------------------------+\n","|label|prediction|probability                               |rawPrediction                           |\n","+-----+----------+------------------------------------------+----------------------------------------+\n","|1.0  |1.0       |[3.4080155213727247E-9,0.9999999965919845]|[-19.497135670188293,19.497135670188293]|\n","|1.0  |1.0       |[2.879363958860322E-9,0.999999997120636]  |[-19.665696411927563,19.665696411927563]|\n","|1.0  |1.0       |[3.5289505204045916E-9,0.9999999964710495]|[-19.462265309677086,19.462265309677086]|\n","|1.0  |1.0       |[5.337068853688215E-9,0.9999999946629311] |[-19.048589233143787,19.048589233143787]|\n","|1.0  |1.0       |[6.915837366589959E-9,0.9999999930841627] |[-18.789451778016684,18.789451778016684]|\n","|1.0  |1.0       |[4.754471197700219E-9,0.9999999952455288] |[-19.16418035211592,19.16418035211592]  |\n","|1.0  |1.0       |[7.3168874515820805E-9,0.9999999926831126]|[-18.733080803616865,18.733080803616865]|\n","|1.0  |1.0       |[9.451378530285168E-9,0.9999999905486214] |[-18.47710522040763,18.47710522040763]  |\n","|0.0  |0.0       |[0.999999993979062,6.020937970419027E-9]  |[18.928022768750157,-18.928022768750157]|\n","|0.0  |0.0       |[0.999999987873826,1.2126173953852515E-8] |[18.227899566519987,-18.227899566519987]|\n","|0.0  |0.0       |[0.9999999949731984,5.026801641605516E-9] |[19.10848192039887,-19.10848192039887]  |\n","|0.0  |0.0       |[0.9999999917227779,8.277222129926542E-9] |[18.60975841383184,-18.60975841383184]  |\n","|0.0  |0.0       |[0.9999999943376006,5.66239943822211E-9]  |[18.989418103125054,-18.989418103125054]|\n","|1.0  |1.0       |[5.3218004906198344E-9,0.9999999946781996]|[-19.051454147441873,19.051454147441873]|\n","|0.0  |0.0       |[0.9999999952754255,4.724574509396007E-9] |[19.170488341992012,-19.170488341992012]|\n","|0.0  |0.0       |[0.999999993468857,6.531142959786962E-9]  |[18.846683888979832,-18.846683888979832]|\n","|0.0  |0.0       |[0.9999999942744076,5.725592444605354E-9] |[18.97831982330743,-18.97831982330743]  |\n","|0.0  |0.0       |[0.9999999921573309,7.842669069724195E-9] |[18.663686599231013,-18.663686599231013]|\n","|0.0  |0.0       |[0.9999999944668201,5.533179914252173E-9] |[19.01250317370493,-19.01250317370493]  |\n","|1.0  |1.0       |[6.19638616377187E-9,0.9999999938036138]  |[-18.899299585448695,18.899299585448695]|\n","+-----+----------+------------------------------------------+----------------------------------------+\n","only showing top 20 rows\n","\n"]}],"source":["pred = model.transform(weighted_churn_test)\n","pred.select('label', 'prediction', 'probability', 'rawPrediction').show(truncate=False)"]},{"cell_type":"markdown","metadata":{},"source":["For all performance measures taken, our model achieves an almost perfect score. This means that indeed the main difficulty our previous model had was dealing with imbalanced data. Our prioritized metrics, AUCROC and weighted recall are very high. Other metrics that involve recall, such as AUCPR and F1-score, are also top-notch.\n","\n","Our other option for furthering the analysis was to perform feature selection. However, since our performance is as good as it can be, we will not follow that path.\n","\n","We conclude that the final model is able to predict if customers are going to close their account on the bank with great precision and accuracy. Especially considering a dataset with reasonable class balance or if proper class weights are supplied to the model."]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model evaluation on testing set:\n","Area under the ROC curve: 0.999999353476004\n","Area under the PR curve: 0.9999975699962578\n","Accuracy: 1.0\n","Weighted precision: 1.0\n","Weighted recall: 1.0\n","F1-score: 1.0\n"]}],"source":["binary_evaluator = BinaryClassificationEvaluator(metricName='areaUnderROC')\n","print(\"Model evaluation on testing set:\")\n","print(\"Area under the ROC curve:\", binary_evaluator.evaluate(pred))\n","binary_evaluator.setMetricName('areaUnderPR')\n","print(\"Area under the PR curve:\", binary_evaluator.evaluate(pred))\n","\n","multiclass_evaluator = MulticlassClassificationEvaluator(metricName='accuracy')\n","print(\"Accuracy:\", multiclass_evaluator.evaluate(pred))\n","multiclass_evaluator.setMetricName('weightedPrecision')\n","print(\"Weighted precision:\", multiclass_evaluator.evaluate(pred))\n","multiclass_evaluator.setMetricName('weightedRecall')\n","print(\"Weighted recall:\", multiclass_evaluator.evaluate(pred))\n","multiclass_evaluator.setMetricName('f1')\n","print(\"F1-score:\", multiclass_evaluator.evaluate(pred))"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":4},"notebookName":"1.0-gdfs-logistic-class","notebookOrigID":1358032175807098,"widgets":{}},"kernelspec":{"display_name":"Python 3.8.13 ('bf_minhashing')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"b39e83d7d937e7ce85387f84ef071986d0096be4afb1ae7c3fe160a424884580"}}},"nbformat":4,"nbformat_minor":0}
